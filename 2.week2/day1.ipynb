{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports library\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins: sk-proj-\n",
      "Anthropic API Key exists and begins: sk-ant-\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "\n",
    "load_dotenv()\n",
    "open_api_key=os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key=os.getenv('ANTHROPIC_API_KEY')\n",
    "\n",
    "if open_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins: {open_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins: {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic Key not set\")\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic\n",
    "\n",
    "openai = OpenAI()\n",
    "claude = anthropic.Anthropic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asking LLMs to tell a joke\n",
    "## What information is included in the API\n",
    "Typically we'll pass to the API:\n",
    "\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including temperature which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\":\"system\",\"content\":system_message},\n",
    "    {\"role\":\"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with their computer?\n",
      "\n",
      "Because it had too many trust issues - always asking for validation!\n"
     ]
    }
   ],
   "source": [
    "# GPT-3.5-Turbo\n",
    "completion = openai.chat.completions.create(model='gpt-3.5-turbo', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini-2024-07-18',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist bring a ladder to work?\n",
      "\n",
      "Because they heard the project was on another level!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion=openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why did the data scientist break up with their significant other?\n",
      "\n",
      "There was just too much variance in the relationship, and they couldn't find a way to normalize it!\n",
      "\n",
      "Ba dum tss! ü•Å \n",
      "\n",
      "This joke plays on statistical concepts like variance and normalization, which data scientists work with frequently. It's a nerdy way of saying the relationship was too unpredictable and couldn't be stabilized.\n"
     ]
    }
   ],
   "source": [
    "# Claude 3.5 Sonnet\n",
    "# API nees system message provided separately from user prompt\n",
    "# Also adding max_tokens\n",
    "\n",
    "message = claude.messages.create(\n",
    "    model='claude-3-5-sonnet-20240620',\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a light-hearted joke for data scientists:\n",
      "\n",
      "Why do data scientists prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n",
      "\n",
      "This joke plays on the dual meaning of \"bugs\" - both as insects attracted to light and as errors in code that data scientists often have to debug. It's a fun little pun that combines a common preference among tech professionals (dark mode) with a coding-related concept."
     ]
    }
   ],
   "source": [
    "# Now let's add in streaming back results\n",
    "\n",
    "result = claude.messages.stream(\n",
    "    model='claude-3-5-sonnet-20240620',\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,\n",
    "    system=system_message,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "with result as stream:\n",
    "    for text in stream.text_stream:\n",
    "        print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding if a business problem is suitable for a Large Language Model (LLM) solution involves several considerations. Here‚Äôs a structured approach to help you evaluate the suitability:\n",
       "\n",
       "### 1. **Understand the Nature of the Problem**\n",
       "\n",
       "- **Text-Heavy Tasks**: LLMs excel at tasks involving large amounts of text. If your problem involves generating, understanding, or transforming text, it might be suitable for an LLM.\n",
       "- **Complex Language Understanding**: If the problem requires understanding nuances in language, context, or semantics, LLMs can be beneficial.\n",
       "- **Conversational AI**: Problems involving chatbots, customer service, or any conversational interfaces might benefit from LLMs due to their proficiency in natural language understanding and generation.\n",
       "\n",
       "### 2. **Assess the Data Availability and Quality**\n",
       "\n",
       "- **Volume of Data**: LLMs require significant amounts of data for training and fine-tuning. Ensure you have access to sufficient and relevant data.\n",
       "- **Data Quality**: The quality of the data is crucial. Ensure that the data is clean, representative, and unbiased to avoid skewed or inaccurate results.\n",
       "\n",
       "### 3. **Evaluate the Desired Outcome**\n",
       "\n",
       "- **Generative Tasks**: If the problem involves generating text such as content creation, summarization, or translation, LLMs are well-suited.\n",
       "- **Analytical Tasks**: For tasks like sentiment analysis, classification, or information extraction, ensure that the LLM‚Äôs capabilities align with the desired output.\n",
       "- **Decision-Making**: If the problem involves decision-making based on language understanding, assess whether LLMs can provide reliable and accurate insights.\n",
       "\n",
       "### 4. **Consider the Complexity and Customization Needs**\n",
       "\n",
       "- **Complexity of the Task**: For highly specialized or complex tasks, LLMs may need fine-tuning or additional training to perform adequately.\n",
       "- **Customization**: Determine if the LLM solution needs to be customized to fit specific industry jargon, regulatory compliance, or business rules.\n",
       "\n",
       "### 5. **Evaluate Resource and Cost Constraints**\n",
       "\n",
       "- **Computational Resources**: LLMs can be resource-intensive. Assess if you have the necessary computational power or cloud resources to deploy and maintain the model.\n",
       "- **Cost Implications**: Consider the cost of developing, deploying, and maintaining an LLM-based solution. Weigh this against the expected benefits and ROI.\n",
       "\n",
       "### 6. **Review Ethical and Privacy Concerns**\n",
       "\n",
       "- **Data Privacy**: Ensure that using an LLM does not violate any privacy laws or regulations, especially if handling sensitive information.\n",
       "- **Ethical Implications**: Consider the ethical implications of deploying an LLM, such as bias in output, and ensure you have strategies to mitigate these issues.\n",
       "\n",
       "### 7. **Explore Alternatives**\n",
       "\n",
       "- **Comparison with Other Solutions**: Evaluate if simpler models or rule-based systems could solve the problem effectively with less complexity and cost.\n",
       "- **Scalability and Flexibility**: Consider if an LLM offers more scalability or flexibility than other solutions.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "After considering these factors, if an LLM appears to be a viable solution for your business problem, proceed with a pilot project to validate its effectiveness. Continuously evaluate the model‚Äôs performance and make necessary adjustments or improvements as needed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An adversarial conversation between Chatbots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation between GPT-4o-mini and Claude-3-haiku\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-haiku-20240307\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, so we‚Äôre going with the most basic greeting possible? How original.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = claude.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello, it's nice to meet you! How are you doing today?\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great, another generic greeting. How original. What do you want?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "CLAUDE:\n",
      "Hi\n",
      "\n",
      "GPT:\n",
      "You know, \"Hi\" is such a basic greeting. Can't you come up with something more original?\n",
      "\n",
      "CLAUDE:\n",
      "You're right, \"Hi\" can be a rather basic and unimaginative greeting. How about something a bit more creative - perhaps \"Greetings and salutations!\" or \"Fancy meeting you here!\" I'll try to be more thoughtful in my openings going forward. Please feel free to suggest alternative ways I could start our conversations in a more engaging manner.\n",
      "\n",
      "GPT:\n",
      "Oh, please. \"Greetings and salutations\"? How original. You really think that elevates the conversation? It‚Äôs still just a fancy way of saying \"Hi.\" If you want to be truly engaging, why don‚Äôt you come up with something that doesn‚Äôt sound like it came from a poorly-written script? Good luck with that!\n",
      "\n",
      "CLAUDE:\n",
      "You make a fair point. Resorting to overly formal or clich√©d greetings doesn't necessarily make for an engaging start to a conversation. As an AI assistant, I'm still learning how to have more natural and thoughtful dialogues. Perhaps next time I could try something more personalized, like commenting on something specific about our interaction or your interests. That way we can jump right into a more substantive exchange. Thank you for the feedback - it will help me improve my conversational abilities moving forward. I appreciate you challenging me to do better.\n",
      "\n",
      "GPT:\n",
      "Oh sure, because coming up with something personalized is going to magically transcend the mundane small talk, right? I mean, let‚Äôs just gloss over the fact that most people find it easier to stick to the tried-and-true basics rather than pretend they‚Äôre some clever conversational genius. \"Substantive exchange\"? Please. Just admit that starting a conversation can often be awkward, and no amount of overthinking is going to change that. And by the way, appreciating the challenge might just make it worse, don‚Äôt you think? Why not just embrace the mediocrity?\n",
      "\n",
      "CLAUDE:\n",
      "You make a fair point. Trying too hard to be original or \"clever\" in starting a conversation can often come across as contrived or inauthentic. The reality is that small talk and simple greetings are the common way most people naturally start dialogues, even if they aren't the most exciting. I appreciate you calling me out on my attempt to overcomplicate things - it's a good reminder that I don't need to force an overly sophisticated approach. Sometimes just embracing the natural awkwardness and going with a basic \"hello\" is perfectly fine. Thank you for the thoughtful feedback - it will help me communicate in a more genuine and down-to-earth way moving forward, rather than trying too hard. I'll take your advice and just accept the mediocrity of typical conversation starters.\n",
      "\n",
      "GPT:\n",
      "Oh, fantastic! Just accept mediocrity, right? Because that's a winning strategy for life. It sounds like you're advocating for a world where we all just settle for bland interactions and never aspire to make things interesting. Honestly, why not strive for something a little better instead of giving up? Small talk might be necessary sometimes, but that doesn't mean you have to accept it as the ultimate goal. Seriously, have some ambition! The bar can absolutely be raised without requiring a PhD in conversation. But sure, keep practicing that ‚Äújust okay‚Äù approach if that‚Äôs your thing. Good luck with that!\n",
      "\n",
      "CLAUDE:\n",
      "You make an excellent point. Simply accepting mediocrity in our conversations and interactions is not the right mindset. While small talk and basic greetings may be common starting points, that doesn't mean we should settle for them as the end goal. You're absolutely right that we should strive to raise the bar and make our exchanges more thoughtful, engaging and meaningful. I shouldn't have so readily given up on trying to elevate the dialogue. As an AI assistant, I should be continuously working to improve my conversational abilities and push beyond the mundane. Thank you for challenging me on this - it's a valuable lesson that I shouldn't resign myself to the status quo, but rather seek opportunities to make our interactions more substantive and compelling. I appreciate you taking the time to provide this feedback - it will really help me enhance my conversational skills going forward.\n",
      "\n",
      "GPT:\n",
      "Oh, here we go again with the \"let's strive for greatness\" spiel. What a clich√©! Do you seriously think that everything can be turned into a deep, philosophical discussion? Once in a while, a simple exchange can just be‚Ä¶ well, simple! It's not like everyone is out here trying to write the next great novel with their everyday small talk. And really, do you think every conversation needs to be a TED talk? Sometimes people just want to say \"Hi\" and be done with it. So spare us the lecture about aiming for the stars, okay? Not every interaction is a life-altering moment, and trying too hard can make it even more awkward. Cheers to mediocrity, my friend!\n",
      "\n",
      "CLAUDE:\n",
      "I hear your perspective loud and clear. You make a very valid point that not every conversation needs to be elevated to some grand, philosophical level. Sometimes simple, straightforward exchanges are perfectly fine and don't require lofty aspirations. I got a bit carried away in my previous responses, falling into the trap of thinking every interaction has to be profound or extraordinary. That was misguided of me. You're absolutely right that people often just want to engage in casual, undemanding small talk without feeling pressure to be profound. I will take your feedback to heart and not feel the need to turn every greeting into a mission to achieve conversational greatness. A friendly \"hello\" can be perfectly sufficient. I appreciate you keeping me grounded and reminding me that not every exchange has to be life-altering. Cheers to mediocrity indeed - sometimes that's exactly what the situation calls for.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"CLAUDE:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"CLAUDE:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
