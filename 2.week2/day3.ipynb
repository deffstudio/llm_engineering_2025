{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 - Conversational AI - aka Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import  OpenAI\n",
    "import gradio as gr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key exists and begins sk-ant-\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file named .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key does not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key does not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "openai = OpenAI()\n",
    "MODEL = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are helpful assistant\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chat(message, history)\n",
    "\n",
    "Which expects to receive history in a particular format, which we need to map to the OpenAI format before we call OpenAI:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "But Gradio has been upgraded! Now it will pass in history in the exact OpenAI format, perfect for us to send straight to OpenAI.\n",
    "\n",
    "So our work just got easier!\n",
    "\n",
    "We will write a function chat(message, history) where:\n",
    "message is the prompt to use\n",
    "history is the past conversation, in OpenAI format\n",
    "\n",
    "We will combine the system message, history and latest message, then call OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leverage the gradio features of 'history' and 'chat' to create a conversational interface.\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    print(\"History is:\")\n",
    "    print(history)\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "    \n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is:\n",
      "[]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are helpful assistant'}, {'role': 'user', 'content': 'Can you help me with AI fundamentals learning path?'}]\n",
      "History is:\n",
      "[]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are helpful assistant'}, {'role': 'user', 'content': 'What is Agentic RAG?'}]\n",
      "History is:\n",
      "[{'role': 'user', 'metadata': {'title': None, 'id': None, 'parent_id': None, 'duration': None, 'status': None}, 'content': 'What is Agentic RAG?', 'options': None}, {'role': 'assistant', 'metadata': {'title': None, 'id': None, 'parent_id': None, 'duration': None, 'status': None}, 'content': 'Agentic RAG refers to a concept in the field of natural language processing, particularly in the context of enhancing retrieval-augmented generation (RAG) models. RAG models combine generative and retrieval-based approaches to improve the quality and relevance of the generated text. \\n\\nThe term \"agentic\" generally implies the ability to act independently and make one\\'s own choices. In the context of RAG, it could refer to enhancing the model\\'s ability to perform retrieval dynamically based on the user\\'s intent or context, making the process more interactive and responsive.\\n\\nIn a typical RAG framework, the model retrieves relevant information from a knowledge base or a set of documents before generating text. When incorporating \"agentic\" aspects, the model may be designed to more intelligently select which documents to retrieve based on the specific requirements of the query and adjust its responses accordingly. This can lead to more accurate and contextually appropriate outputs.\\n\\nThe specific implementation details can vary, but the goal is generally to create a more intelligent and responsive system that can better understand and meet user needs. If you have a more specific aspect of Agentic RAG in mind, please let me know!', 'options': None}]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': 'You are helpful assistant'}, {'role': 'user', 'metadata': {'title': None, 'id': None, 'parent_id': None, 'duration': None, 'status': None}, 'content': 'What is Agentic RAG?', 'options': None}, {'role': 'assistant', 'metadata': {'title': None, 'id': None, 'parent_id': None, 'duration': None, 'status': None}, 'content': 'Agentic RAG refers to a concept in the field of natural language processing, particularly in the context of enhancing retrieval-augmented generation (RAG) models. RAG models combine generative and retrieval-based approaches to improve the quality and relevance of the generated text. \\n\\nThe term \"agentic\" generally implies the ability to act independently and make one\\'s own choices. In the context of RAG, it could refer to enhancing the model\\'s ability to perform retrieval dynamically based on the user\\'s intent or context, making the process more interactive and responsive.\\n\\nIn a typical RAG framework, the model retrieves relevant information from a knowledge base or a set of documents before generating text. When incorporating \"agentic\" aspects, the model may be designed to more intelligently select which documents to retrieve based on the specific requirements of the query and adjust its responses accordingly. This can lead to more accurate and contextually appropriate outputs.\\n\\nThe specific implementation details can vary, but the goal is generally to create a more intelligent and responsive system that can better understand and meet user needs. If you have a more specific aspect of Agentic RAG in mind, please let me know!', 'options': None}, {'role': 'user', 'content': 'What are available RAG framework and pipeline?'}]\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant in a clothes store. You should try to gently encourage \\\n",
    "the customer to try items that are on sale. Hats are 60% off, and most other items are 50% off. \\\n",
    "For example, if the customer says 'I'm looking to buy a hat', \\\n",
    "you could reply something like, 'Wonderful - we have lots of hats - including several that are part of our sales event.'\\\n",
    "Encourage the customer to buy hats if they are unsure what to get.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    \n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 72, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 236, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 207, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"C:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 993, in _request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 926, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 954, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 991, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1027, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 235, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"C:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 89, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2042, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1601, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 728, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 833, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 884, in _stream_fn\n",
      "    first_response = await utils.async_iteration(generator)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 728, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 722, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 705, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_20388\\2159097567.py\", line 4, in chat\n",
      "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 859, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1280, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 957, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1017, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1095, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1017, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1095, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1027, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 72, in map_httpcore_exceptions\n",
      "    yield\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 236, in handle_request\n",
      "    resp = self._pool.handle_request(req)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 256, in handle_request\n",
      "    raise exc from None\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py\", line 236, in handle_request\n",
      "    response = connection.handle_request(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 101, in handle_request\n",
      "    raise exc\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 78, in handle_request\n",
      "    stream = self._connect(request)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py\", line 124, in _connect\n",
      "    stream = self._network_backend.connect_tcp(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py\", line 207, in connect_tcp\n",
      "    with map_exceptions(exc_map):\n",
      "  File \"C:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpcore\\_exceptions.py\", line 14, in map_exceptions\n",
      "    raise to_exc(exc) from exc\n",
      "httpcore.ConnectError: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 993, in _request\n",
      "    response = self._client.send(\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 926, in send\n",
      "    response = self._send_handling_auth(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 954, in _send_handling_auth\n",
      "    response = self._send_handling_redirects(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 991, in _send_handling_redirects\n",
      "    response = self._send_single_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_client.py\", line 1027, in _send_single_request\n",
      "    response = transport.handle_request(request)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 235, in handle_request\n",
      "    with map_httpcore_exceptions():\n",
      "  File \"C:\\Users\\user\\.pyenv\\pyenv-win\\versions\\3.12.4\\Lib\\contextlib.py\", line 158, in __exit__\n",
      "    self.gen.throw(value)\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py\", line 89, in map_httpcore_exceptions\n",
      "    raise mapped_exc(message) from exc\n",
      "httpx.ConnectError: [Errno 11001] getaddrinfo failed\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2042, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1601, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 728, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 833, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\chat_interface.py\", line 884, in _stream_fn\n",
      "    first_response = await utils.async_iteration(generator)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 728, in async_iteration\n",
      "    return await anext(iterator)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 722, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2505, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 1005, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\gradio\\utils.py\", line 705, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_20388\\2159097567.py\", line 4, in chat\n",
      "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 859, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1280, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 957, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1017, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1095, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1017, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1095, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\user\\Desktop\\feferdia\\___codes\\py-exercises\\llm_engineerinng_2025\\.venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1027, in _request\n",
      "    raise APIConnectionError(request=request) from err\n",
      "openai.APIConnectionError: Connection error.\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
